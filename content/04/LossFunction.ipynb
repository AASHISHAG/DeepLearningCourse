{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss funcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone makes mistakes. The wise are not people who never make mistakes, but those who forgive themselves and learn from their mistakes. —— Ajahn Brahm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you take a math test, the teacher will mark you down for the mistakes you made. Your friend ask you how's your test? You may say: well, not bad. I made two mistakes and deducted 6 points. Then you learn from your mistakes and perform better the next time. The score you get on a test is an indicator. The fewer errors you make, the fewer points you deduct, the higher your score.\n",
    "\n",
    "For neural network, there is such an indicator too. It's called **loss function** (or cost function). Many function can be used as loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about which function should we use as loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows each node's actual value and target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Node | Actual value by network | Target training values |\n",
    "| ---- | ----------------------- | ---------------------- |\n",
    "| 1    | 0.6                     | 0.5                    |\n",
    "| 2    | 0.6                     | 0.7                    |\n",
    "| 3    | 0.7                     | 0.85                   |\n",
    "| 4    | 0.95                    | 0.8                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that comes to mind is to use the difference between the target training values (t) and the actual value(y) calculated by neural network directly.\n",
    "\n",
    "\n",
    "$$E = \\sum_{i=1}^{n}\\left ( y_{i} - t_{i} \\right )$$\n",
    "\n",
    "\n",
    "Here, $n$ is the number of output nodes. $y_{i}$ is the output value of neural network and $t_{i}$ is the target training value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this loss function, each node's error is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Node | Actual value by network | Target training values | Error |\n",
    "| ---- | ----------------------- | ---------------------- | ----- |\n",
    "| 1    | 0.6                     | 0.5                    | 0.1   |\n",
    "| 2    | 0.6                     | 0.7                    | -0.1  |\n",
    "| 3    | 0.7                     | 0.85                   | -0.15 |\n",
    "| 4    | 0.95                    | 0.8                    | 0.15  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we sum them up:\n",
    "\n",
    "$$E = 0.1 + (-0.1) + (-0.15) + (0.15)$$\n",
    "$$E = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the positive and negative errors cancel each other out. So the total error become 0 which suggest the neural network makes no mistakes. However, that's not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can use the mean of absolute value of the difference. It's called The Mean Absolute Error (MAE).\n",
    "\n",
    "$$E =\\frac{1}{n} \\sum_{i=1}^{n}\\left | y_{i} - t_{i} \\right |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Node | Actual value by network | Target training values | Error |\n",
    "| ---- | ----------------------- | ---------------------- | ----- |\n",
    "| 1    | 0.6                     | 0.5                    | 0.1   |\n",
    "| 2    | 0.6                     | 0.7                    | 0.1  |\n",
    "| 3    | 0.7                     | 0.85                   | 0.15 |\n",
    "| 4    | 0.95                    | 0.8                    | 0.15  |\n",
    "\n",
    "$$E = 0.1 + 0.1 + 0.15 + 0.15$$\n",
    "$$E = 0.6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, the above problem will be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mean_absolute_error(y, t):\n",
    "    mse = 1/len(y) * np.sum(abs(y-t))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999997\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0.6, 0.6, 0.7, 0.95])\n",
    "t = np.array([0.5, 0.7, 0.85, 0.8])\n",
    "e = mean_absolute_error(y, t)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more common loss function: Mean Squared Error(MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = \\frac{1}{n}\\sum_{i=1}^{k}\\left ( y_{k} - t_{k} \\right )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    mse = 1/len(y) * np.sum((y-t) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016249999999999994\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0.6, 0.6, 0.7, 0.95])\n",
    "t = np.array([0.5, 0.7, 0.85, 0.8])\n",
    "e = mean_squared_error(y, t)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we use MSE as loss function more than MAE. It relates to the gradient descent we'll talk about next. MAE isn’t continuous near the minimum which makes gradient descent not work so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
